# -*- coding: utf-8 -*-
"""Tarea_4_Autoencoder.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IypG74pLvuVHynG4Wl8umInxaRAsEReP

# Universidad de O'Higgins

## Escuela de Ingeniería
## COM4402: Introducción a Inteligencia Artificial

### **Tarea 4: Aprendizaje Profundo Avanzado**

### **Autoencoders**

### Estudiante: Mackarena Ulloa Jara

Un autoencoder es un tipo especial de red neuronal que se entrena para copiar su entrada en su salida. Por ejemplo, dada una imagen de un dígito escrito a mano, un autoencoder primero codifica la imagen en una representación latente de menor dimensión, y luego decodifica la representación latente de vuelta a una imagen. Un autoencoder aprende a comprimir los datos minimizando el error de reconstrucción.

Para saber más sobre los autoencoders, le recomendamos el capítulo 14 de [Deep Learning](https://www.deeplearningbook.org/) de Ian Goodfellow, Yoshua Bengio y Aaron Courville.

## Importar TensorFlow y otras librerías
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model

"""## Cargar el conjunto de datos
Para empezar, entrenaremos el autoencoder básico utilizando el conjunto de datos Fashion MNIST. Cada imagen de este conjunto de datos tiene 28x28 píxeles.
"""

(x_train, _), (x_test, _) = fashion_mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

print (x_train.shape)
print (x_test.shape)

"""## Primer ejemplo: Autoencoder básico
![Resultados del autoencoder básico](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/intro_autoencoder_result.png?raw=1)

Define un autoencoder con dos capas densas: un `encoder`, que comprime las imágenes en un vector latente de 64 dimensiones, y un `decoder`, que reconstruye la imagen original a partir del espacio latente.

Para definir tu modelo, se utiliza [Keras Model Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models).
"""

class Autoencoder(Model):
  def __init__(self, latent_dim, shape):
    super(Autoencoder, self).__init__()
    self.latent_dim = latent_dim
    self.shape = shape
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(latent_dim, activation='relu'),
    ])
    self.decoder = tf.keras.Sequential([
      layers.Dense(tf.math.reduce_prod(shape), activation='sigmoid'),
      layers.Reshape(shape)
    ])

  def call(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded


shape = x_test.shape[1:]
latent_dim = 64
autoencoder = Autoencoder(latent_dim, shape)

autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())

"""Entrena el modelo utilizando `x_train` como entrada y como objetivo. El `encoder` aprenderá a comprimir el conjunto de datos de 784 dimensiones al espacio latente, y el `decoder` aprenderá a reconstruir las imágenes originales."""

autoencoder.fit(x_train, x_train,
                epochs=10,
                shuffle=True,
                validation_data=(x_test, x_test))

"""Ahora que el modelo está entrenado, vamos a probarlo codificando y decodificando imágenes del conjunto de prueba."""

encoded_imgs = autoencoder.encoder(x_test).numpy()
decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
  # display original
  ax = plt.subplot(2, n, i + 1)
  plt.imshow(x_test[i])
  plt.title("original")
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

  # display reconstruction
  ax = plt.subplot(2, n, i + 1 + n)
  plt.imshow(decoded_imgs[i])
  plt.title("reconstructed")
  plt.gray()
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
plt.show()

from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Flatten, Dense, Reshape

# Definir entrada
input_img = Input(shape=(28, 28))

# Encoder
flatten_layer = Flatten()(input_img)
encoded = Dense(64, activation='relu')(flatten_layer)

# Decoder
decoded = Dense(784, activation='sigmoid')(encoded)
reshape_layer = Reshape((28, 28))(decoded)

# Definir modelo solo con las capas relevantes
autoencoder_model = Model(input_img, reshape_layer)

# Visualizar arquitectura
plot_model(autoencoder_model, to_file="autoencoder_architecture.png", show_shapes=True)

"""## Segundo ejemplo: Eliminación de ruido de imágenes


![Resultados de la eliminación de ruido de imágenes](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/image_denoise_fmnist_results.png?raw=1)

También se puede entrenar un autoencoder para eliminar el ruido de las imágenes. En la siguiente sección, crearás una versión ruidosa del conjunto de datos Fashion MNIST aplicando ruido aleatorio a cada imagen. A continuación, entrenaremos un autoencoder utilizando la imagen ruidosa como entrada y la imagen original como objetivo.

Vamos a reimportar el conjunto de datos para omitir las modificaciones realizadas anteriormente.
"""

(x_train, _), (x_test, _) = fashion_mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

x_train = x_train[..., tf.newaxis]
x_test = x_test[..., tf.newaxis]

print(x_train.shape)

"""Añadir ruido aleatorio a las imágenes"""

noise_factor = 0.2
x_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape)
x_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape)

x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)
x_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)

"""Grafique las imágenes ruidosas."""

n = 10
plt.figure(figsize=(20, 2))
for i in range(n):
    ax = plt.subplot(1, n, i + 1)
    plt.title("original + noise")
    plt.imshow(tf.squeeze(x_test_noisy[i]))
    plt.gray()
plt.show()

"""### Definir un autoencoder convolucional

En este ejemplo, entrenarás un autoencoder convolucional usando capas [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) en el `encoder`, y capas [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) en el `decoder`.
"""

class Denoise(Model):
  def __init__(self):
    super(Denoise, self).__init__()
    self.encoder = tf.keras.Sequential([
      layers.Input(shape=(28, 28, 1)),
      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),
      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])

    self.decoder = tf.keras.Sequential([
      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),
      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),
      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])

  def call(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return decoded

autoencoder = Denoise()

autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())

autoencoder.fit(x_train_noisy, x_train,
                epochs=10,
                shuffle=True,
                validation_data=(x_test_noisy, x_test))

"""Veamos un resumen del encoder. Observa cómo las imágenes se reducen de 28x28 a 7x7."""

autoencoder.encoder.summary()

"""El decoder aumenta la resolución de las imágenes de 7x7 a 28x28."""

autoencoder.decoder.summary()

"""Representación gráfica de las imágenes ruidosas y de las imágenes sin ruido producidas por el autoencoder."""

encoded_imgs = autoencoder.encoder(x_test_noisy).numpy()
decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()

n = 10
plt.figure(figsize=(20, 4))
for i in range(n):

    # display original + noise
    ax = plt.subplot(2, n, i + 1)
    plt.title("original + noise")
    plt.imshow(tf.squeeze(x_test_noisy[i]))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # display reconstruction
    bx = plt.subplot(2, n, i + n + 1)
    plt.title("reconstructed")
    plt.imshow(tf.squeeze(decoded_imgs[i]))
    plt.gray()
    bx.get_xaxis().set_visible(False)
    bx.get_yaxis().set_visible(False)
plt.show()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose
import tensorflow as tf
from tensorflow.keras.utils import plot_model

# Definir la arquitectura del autoencoder convolucional
inputs = Input(shape=(28, 28, 1))
encoder = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2)(inputs)
encoder = Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)(encoder)

decoder = Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same')(encoder)
decoder = Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same')(decoder)
decoded = Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')(decoder)

# Crear el modelo
autoencoder = Model(inputs, decoded)

# Visualizar la arquitectura
plot_model(autoencoder, to_file='autoencoder_architecture2.png', show_shapes=True)